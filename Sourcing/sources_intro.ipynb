{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ChA5VqEqUTG"
      },
      "source": [
        "#\n",
        "# Accessing Structured and Unstructured Data Sources in Data Science\n",
        "\n",
        "## Introduction to Data Sources\n",
        "Data for data science projects comes from a variety of sources, each with unique characteristics. These sources can be broadly categorized into:\n",
        "- **Structured Data**: Highly organized data stored in tables (e.g., databases) or spreadsheets\n",
        "- **Unstructured Data**: Free-form data that lacks a pre-defined structure (e.g., text documents, images, web pages)\n",
        "- **Streams: logs, real-time events, sockets\n",
        "\n",
        "This session will cover how to access different types of data sources, including databases, web scraping, document processing, streaming data, and real-time APIs.\n",
        "\n",
        "## Accessing Structured Data: Databases\n",
        "Databases are a common source of structured data. SQL databases like MySQL and PostgreSQL store data in tables with a defined schema, making them ideal for handling large amounts of structured data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pandas beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BXVoSdHwictk",
        "outputId": "4c90115a-fe6b-4d94-cf70-b376945af2e9"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Connect to an in-memory SQLite database\n",
        "conn = sqlite3.connect(':memory:')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# STEP1: Create the 'sales_data' table\n",
        "cursor.execute('''\n",
        "    CREATE TABLE sales_data (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        product_name TEXT NOT NULL,\n",
        "        product_price REAL,\n",
        "        customer_id INTEGER,\n",
        "        timestamp TEXT\n",
        "    )\n",
        "''')\n",
        "\n",
        "# Build some fake data\n",
        "products = [\n",
        "    (\"Laptop\", 1200.00),\n",
        "    (\"Smartphone\", 699.99),\n",
        "    (\"Headphones\", 199.99),\n",
        "    (\"Keyboard\", 49.99),\n",
        "    (\"Mouse\", 29.99),\n",
        "    (\"Monitor\", 299.99),\n",
        "    (\"Tablet\", 329.99),\n",
        "    (\"Smartwatch\", 199.99)\n",
        "]\n",
        "\n",
        "# Function to create a random timestamp within the last year\n",
        "def random_timestamp():\n",
        "    start_date = datetime.now() - timedelta(days=365)\n",
        "    random_date = start_date + timedelta(days=random.randint(0, 365), hours=random.randint(0, 23), minutes=random.randint(0, 59))\n",
        "    return random_date.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Insert fake data into the table\n",
        "for _ in range(50):  # Creating 50 rows of data\n",
        "    product_name, product_price = random.choice(products)\n",
        "    customer_id = random.randint(1000, 2000)  # Fake customer IDs between 1000 and 2000\n",
        "    timestamp = random_timestamp()\n",
        "    cursor.execute(\"INSERT INTO sales_data (product_name, product_price, customer_id, timestamp) VALUES (?, ?, ?, ?)\",\n",
        "                   (product_name, product_price, customer_id, timestamp))\n",
        "\n",
        "# Commit changes\n",
        "conn.commit()\n",
        "\n",
        "# Query and display the data in a DataFrame for easy viewing\n",
        "df = pd.read_sql_query(\"SELECT * FROM sales_data\", conn)\n",
        "df.head(10)  # Display the first 10 rows of the generated data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ2xa4Rvqjk7"
      },
      "source": [
        "# Accessing Unstructured Data: Web Scraping\n",
        "Used to extract data from websites. It is commonly used to gather data for research or analysis when APIs are not available, and (questionably) for training LLM's.\n",
        "\n",
        "## Explanation\n",
        "\n",
        "- __Send Request__: requests.get(url) sends a GET request to the website.\n",
        "- __Parse Content__:  The BeautifulSoup parser ('html.parser') processes the page’s HTML content for easy access to elements.\n",
        "- __Locate Data__:\n",
        "  - Locate books using via html tag:\n",
        " `<article class='product_pod'>`\n",
        "  - Within each product_pod, we find the title using h3.a['title']\n",
        "  - We retrieve the price using find('p', class_='price_color').text\n",
        "\n",
        "This example extracts and prints each book’s title and price, demonstrating how to gather structured data from a website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnlaM1S5ql8E",
        "outputId": "0832c9c9-10f5-44ed-f421-f805bff26681"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Target URL -- actually provided online to test scrapers...\n",
        "theURL = 'http://books.toscrape.com/'\n",
        "\n",
        "# Send a request to get the top level page\n",
        "response = requests.get(theURL)\n",
        "#print(response.content) #show web content...\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the page content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all book containers\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # Extract book titles and prices\n",
        "    for book in books:\n",
        "        # Get the title\n",
        "        title = book.h3.a['title']\n",
        "\n",
        "        # Get the price\n",
        "        price = book.find('p', class_='price_color').text\n",
        "\n",
        "        # Print the extracted data\n",
        "        print(f\"Title: {title}, Price: {price}\")\n",
        "else:\n",
        "    print(\"Failed to retrieve the webpage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyKYlG4Sqmmw"
      },
      "source": [
        "# Working with Documents (e.g., PDFs and Word Documents)\n",
        "Document processing is crucial for extracting information from unstructured data in formats like PDFs, Word documents, or text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZxZCACnOqoLy",
        "outputId": "afbf6e0f-645a-4f8d-83e4-5e7cdce81e1f"
      },
      "outputs": [],
      "source": [
        "%pip install PyMuPDF  # Library for handling PDFs\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "from io import BytesIO\n",
        "\n",
        "# Open a sample PDF\n",
        "# UPDATED: Using the correct URL from the global variable\n",
        "theURL = \"https://images.apple.com/id/environment/pdf/products/iphone/iPhone_15_and_iPhone_15_Plus_PER_Sept2023.pdf\"\n",
        "theResponse = requests.get(theURL)\n",
        "\n",
        "# Check if the request was successful\n",
        "if theResponse.status_code == 200:\n",
        "    theStream = BytesIO(theResponse.content) #load pdf content into stream\n",
        "    thePDF = fitz.open(stream=theStream, filetype=\"pdf\") #make in-memory pdf from stream\n",
        "\n",
        "    theFirstPage = thePDF[0].get_text()\n",
        "    print(\"Text from the first page:\\n\", theFirstPage)\n",
        "else:\n",
        "    print(\"Failed to retrieve the PDF \", theResponse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM5CFlsqqqhB"
      },
      "source": [
        "# Accessing Streaming Data\n",
        "Streaming data is real-time data that flows continuously. Common sources include social media feeds, financial market data, and sensor data.\n",
        "\n",
        "## Example: Simulating a Data Stream with Kafka\n",
        "\n",
        "\tNote: Running Kafka in a Jupyter Notebook requires complex setup. Here’s a simplified example using a generator to simulate streaming data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygO8VsDGqstC",
        "outputId": "358de269-7427-4498-c961-d9d3957676ef"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Simulated streaming data\n",
        "def data_stream():\n",
        "    for i in range(10):  # Generate 5 data points\n",
        "        yield {\"timestamp\": pd.Timestamp.now(), \"value\": i}\n",
        "        time.sleep(1)  # Simulate a 1-second delay\n",
        "\n",
        "# Access the stream\n",
        "for data_point in data_stream():\n",
        "    print(data_point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbj3ecMqq1ZX"
      },
      "source": [
        "# Accessing Real-Time APIs\n",
        "APIs provide access to data over the internet. Real-time APIs are often used in data science for retrieving current data (e.g., weather, stock prices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OQB0Pj6q4x9",
        "outputId": "7848eb4d-78cb-46b1-a5ec-b26f2cac27c5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# API key and endpoint\n",
        "api_key = 'dbfb5afaded4b480dbc5d8983e2b76e2'  # Replace with your API key\n",
        "city = 'London'\n",
        "lat=51.5073219\n",
        "lon=-0.1276474\n",
        "theURL=f\"http://api.openweathermap.org/data/2.5/weather?q={city},uk&APPID={api_key}\"\n",
        "\n",
        "# Make the request\n",
        "response = requests.get(theURL)\n",
        "\n",
        "# Display relevant information\n",
        "data = response.json()\n",
        "theWeather = {\n",
        "    \"City\": city,\n",
        "    \"Lat\": lat,\n",
        "    \"Lon\": lon,\n",
        "    \"Temperature\": data[\"main\"][\"temp\"],\n",
        "    \"Forecast\": data[\"weather\"][0][\"description\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(list(theWeather.items()), columns=['Key', 'Value'])\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He_KVBPcq5kN"
      },
      "source": [
        "# Best Practices for Accessing Data Sources\n",
        "- **Use APIs When Available**: APIs are generally more reliable and structured compared to web scraping\n",
        "- **Handle Errors and Rate Limits**: APIs and web scraping may have rate limits, so handle errors gracefully\n",
        "- **Sanitize and Structure Unstructured Data**: When dealing with unstructured data (e.g., web scraping, PDFs), use text cleaning and preprocessing techniques\n",
        "- **Ensure Data Security and Compliance**: Sensitive data (e.g., customer data) should be accessed securely, following relevant data protection laws\n",
        "- **Right of Conveyance**: You might have the right to consume a data source, but not to store or forward it to others -- confirm your rights!\n",
        "\n",
        "## Transient Storage\n",
        "> Golden Rule of Data: Parallel systems never are\n",
        "\n",
        "In other words, be careful about gathering data from a primary source, and caching in a secondary store.\n",
        "\n",
        "> QUESTION : What problems might this introduce?\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "Accessing data from various sources is essential for data science. By combining structured and unstructured data, data scientists can generate valuable insights and build comprehensive data-driven solutions. Each data source requires different tools and techniques, and understanding how to effectively access and process each one is crucial in data science."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
