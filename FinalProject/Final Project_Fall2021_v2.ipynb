{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSE 200 FINAL PROJECT\n",
    "## Fall 2021\n",
    "### Due Date:  December 3rd, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final project is comprised of two parts: \n",
    "* <b>Part I</b> is a set of coding questions that require the _numpy_ library to analyze the provided dataset.  \n",
    "* <b>Part II</b> is a guided project for you to build your own end-to-end analysis using Python, especially using what you learned on Python _IO_, _pandas_, _matplotlib_ and _scilitlearn_ libraries.  \n",
    "\n",
    "<b>Deliverables</b>: Submit both parts as one notebook via Gradescope by midnight on the due date above along with clear instructions on how to download the datasets you used for Part II and reproduce your results. The notebook should be organized with a clear table of contents on top _(see example in the Pylaski notebook at https://github.com/words-sdsc/wifire/blob/master/pylaski.ipynb)_ and links to the parts/steps outlined. Don't forget to add your name on top as the author of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preliminaries\n",
    "\n",
    "Use numpy to load `prostate_cancer.npy` into a numpy matrix. Print the dataset's shape and the first 5 rows.<br>\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Tuple representing dataset's shape</li>\n",
    "    <li>Matrix representing the first 5 rows</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference\n",
    "column_names = ['id', 'diagnosis_result', 'radius', 'texture', 'perimeter', 'area',\n",
    "                'smoothness', 'compactness', 'symmetry', 'fractal_dimension']\n",
    "diagnosis_encoding = {'Benign': 0, 'Malignant': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('prostate_cancer.npy', allow_pickle=True)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Transformations\n",
    "\n",
    "* The first column is the id of the patient, which isn't relevant for our purposes. Remove that column from the matrix by creating a new matrix composed of the rest of the columns.\n",
    "* The second column is the `diagnosis_result` i.e. **M** for malignant and **B** for benign. Replace **M** with `1` and **B** with `0`\n",
    "* Convert the `dtype` of the resulting array to `np.float64`\n",
    "* As usual, print the shape of the resulting dataset and the first 5 rows.\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Tuple representing dataset's shape</li>\n",
    "    <li>Matrix representing the first 5 rows</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Print the means and standard deviations of each column\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Floats representing the mean of each column</li>\n",
    "    <li>Floats representing the standard deviation of each column</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Print the minimum and maximum values of each column\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Floats representing the minimum value found in each column</li>\n",
    "    <li>Floats representing the maximum value found in each column</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Calculate the `diagnosis_result`-wise means and standard deviations.<br>\n",
    "**Report these values with respect to the actual *name* of the result, for which you must refer to 1.1**\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>For each of the 2 results in the dataset:<ul>\n",
    "        <li>Floats representing the standard deviation of each column for this result</li>\n",
    "        <li>Floats representing the mean of each column for this result</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II : 80%\n",
    "\n",
    "This project is culmination of all you’ve learned in this course! You should expect to spend <b>24-32 total hours</b> on the project. Be sure to read all of the items below before starting.\n",
    "\n",
    "There are a number of steps outlined below, but is critical that you do not view this as an entirely linear process.  Remember that the science component in data science is the creation of a hypothesis based on exploration and testing of that hypothesis through analysis.  You may need to go through many of these steps multiple times before you arrive at meaningful hypothesis or conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find a dataset or datasets\n",
    "\n",
    "Based on your interest, identify a dataset which you will want to examine.  You will find a starting point for where you can find open datasets at the end of this notebook, but feel free to use other datasets you have access to and can publicly share results about. \n",
    " \n",
    "\n",
    "This step may take some time, as you’ll likely look at a number of datasets before you find one (or more) which holds promising data for the kinds of questions you want to ask. You are expected to use at least two interconnected datasets, e.g., two tables in one database or a combination of datasets which you can merge in some meaningful way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLAIN AND INGEST YOUR DATASET IN THIS SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the datasets\n",
    "\n",
    "In this step, you should explore what is present in the data and how the data is organized. You’ll need to determine what common features allow you to merge the datasets.  \n",
    "\n",
    "You are expected to answer the following questions using the _pandas_ library and markdown cells to describe your actions:\n",
    "\n",
    "* Are there quality issues in the dataset (noisy, missing data, etc.)? \n",
    "* What will you need to do to clean and/or transform the raw data for analysis?\n",
    "\n",
    "You are also expected to use the _matplotlib_ library to visually explore the datasets and explain your findings, specifically,\n",
    "\n",
    "* How are the data distributed? \n",
    "* What are some common trends?\n",
    "* What are the relationships between variables in your datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERFORM AND EXPLAIN YOUR EXPLORATORY ANALYSIS IN THIS SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Identify 1-3 research questions and perform analysis\n",
    "\n",
    "Now that you have a better understanding of the data, you will want to form a research question which is interesting to you. The research question should be broad enough to be of interest to a reader but narrow enough that the question can be answered with the data.  Some examples:\n",
    "\n",
    "* __Too Narrow:__  What is the GDP of the U.S. for 2011?  This is just asking for a fact or a single data point.  \n",
    "\n",
    "* __Too Broad:__  What is the primary reason for global poverty?  This could be a Ph.D. thesis and would still be way too broad.  What data will you use to answer this question?  Even if a single dataset offered an answer, would it be defendable given the variety of datasets out there?\n",
    "\n",
    "* __Good:__  Can you use simple sentiment analysis on comments about movies in a movie database to predict its box office earnings?  If you have, or can obtain, data on a variety of movies and you have their box office earnings, this is a question which you can potentially answer well. \n",
    "\n",
    "__Remember__, this course is for learning Python. You will not be graded on the complexity, accuracy or performance of your analytical methods. However, you are expected to use a Python library, e.g., _scikitlearn_, successfully to generate results and explain why you picked the methods you used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERVIEW YOUR QUESTION AND PERFORM YOUR ANALYSIS IN THIS SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Present your findings\n",
    "\n",
    "In this step, you can begin to report your findings.  What did you learn from the data and how do your findings help answer your research question?  Use _matplotlib_ visualizations to present these findings.\n",
    "\n",
    "\n",
    "__Remember:__ Rarely will a single data analysis conclusively answer a research question.  Here, you need to identify possible limitations.  For example, are your results limited to a certain area, city, or country?  Are you making assumptions about the data which may, or may not, be valid (e.g., that students in one term are equally qualified as students in another)?  Document these limitations in a few paragraphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPAND THIS SECTION TO PRESENT YOUR FINDINGS"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
